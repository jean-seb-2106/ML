---
title: "Introduction au machine learning"
author: "Attachés 2024"
format: 
  revealjs:
    logo: logo_cefil.jpg
    slide-number: true
    incremental: true
    embed-resources: true #pour tout mettre dans un seul fichier
    scrollable: true
    theme: serif
editor: visual
---

```{r}
library(kableExtra)
library(dplyr)
library(asta) #pour récupérer la base titanic
library(skimr)
library(tidymodels)
library(rpart.plot)
```

## Vous avez dit "machine learning" ?

![](IA_ML.png){fig-align="center"}

::: notes
L'intelligence artificielle définie par Yann le Cun [@Cun2019] : *l'intelligence artificielle est la capacité pour une machine d'assurer des tâches généralement assurée par les animaux ou les humains : percevoir, raisonner, agir.*

Le machine learning fait partie de l'intelligence artificielle, mais il se distingue de l'IA historique par son mode de fonctionnement.

Le deep learning est un machine learning très puissant, utilisé sur des données très volumineuses (images, vidéos..) et nécessitant de grosses puissances de calculs.
:::

## "La bonne vieille IA"...

![](système_expert.png){fig-align="center"}

::: notes
Il y a "la bonne vieille intelligence artificielle" en français, celle pour laquelle un expert explicite toutes les règles qui sont ensuite traduites en langue informatique. Il faut l'expert à côté de l'informaticien qui code, pour lui donner toutes les règles.

C'est celle qu'on utilise depuis longtemps, depuis les années 50.
:::

## ...VS l'apprentissage automatique

![](machine-learning-entrainement-ia.jpg){fig-align="center"}

::: notes
Avec le machine learning, on donne à la machine une grande quantité de donnée, et elle va trouver 'toute seule' cette règle, c'est pour ça qu'on dit qu'elle apprend. Elle apprend grace à un algorithme et des machines puissantes qui permettent de traiter une (très) grande quantité de donnée pour en faire ressortir un schéma réccurent.
:::

<!-- ## et le Deep learning -->

<!-- ![](reseau_neurones.jpg){fig-align="center"} -->

<!-- ::: notes -->

<!-- Le deep learning est un sous domaine du machine learning : globalement, il veut reproduire le cerveau humain grace à des neurones articifiels. très puissant en reconnaissance d'images par exemple. -->

<!-- ::: -->

## Les différents types d'apprentissage automatique

1.  L'apprentissage supervisé

2.  L'apprentissage non supervisé

3.  L'apprentissage par essai/erreur

::: notes
Dans ce module d'introduction, on va principalement étudier l'apprentissage supervisé. L'apprentissage non supervisé a été vu en partie dans le module sur la modélisation (CAH et K-MEANS). Enfin, le dernier type d'apprentissage ne sera pas vu, c'est celui qui est utilisé pour simuler un joueur de jeu de go ou d'échec par exemple.

la différence entre supervisé et non supervisé est dans la diapo suivante.
:::

------------------------------------------------------------------------

![](supervisé_nonsupervisé.png){fig-align="center"}

::: notes
Dans le non supervisé, les données ne sont pas étiquetés. Le but pour la machine est de détecter les ressemblances entre les individus (cf module sur le clustering).

Dans le supervisé, la machine apprend sur des données étiquetés (avec un Y). L'image a un nom, et le but est de faire en sorte que le modèle soit capable de prédire correctement une nouvelle image.

Cette image vient du site suivant :

<https://www.le-cortex.com/media/articles/lintelligence-artificielle-comment-ca-marche>
:::

## But de l'apprentissage supervisé : la prédiction {.smaller}

::: columns
::: {.column width="50%"}
-   $$
    Y = f(X_1, X_2, ...,X_n)
    $$

-   Le but premier n'est pas d'expliquer mais de prédire.

-   Deux types d'apprentissage supervisé :

    -   Régression

    -   Classification
:::

::: {.column width="50%"}
![](42293349-photo-de-future-femme-prédiction-de-la-boule-de-cristal.jpg)
:::
:::

::: notes
-   Il s'agit de trouver la fonction f qui va le mieux **prédire** Y en fonction de X1, X2...,Xn

-   On distingue la modélisation statistique (cf module 3 de statistique), avec des tests et des hypothèses. Le but est d'expliquer, d'interpréter, de comprendre les données. En machine learning, l'objectif est d'avoir les meilleures performances prédictives, au détriment parfois de l'interprétabilité.
:::

------------------------------------------------------------------------

![](types_apprentissage.png){fig-align="center"}

::: notes
Attention : en machine learning, dans un problème de régression (Y quanti continu), je n'utilise pas nécessairement la régression linéaire (qui est un modèle parmi d'autres). Dans un problème de classification (Y est quali), je peux utiliser une régression logistique comme modèle pour apprendre. Il ne faut pas confondre les méthodes de classification (K-means, CAH) qu'on a vu dans le module de statistique, et la classification supervisée qui consiste à prévoir des données qualitatives. Pour ne pas confondre, on utilise le terme de clustering pour désigner les méthodes d'apprentissage non supervisé.
:::

## Comment la machine apprend ? {.nonsmaller}

-   Le statisticien fixe un cadre d'apprentissage : le modèle

-   Des algorithmes permettent d'entraîner ces modèles sur des données étiquetées. Leur but est de minimiser [l'erreur d'estimation]{.underline}.

-   Les algorithmes sont des répétitions de calculs simples permettant d'approcher de façon itérative le résultat attendu.

::: notes
Un algorithme emblématique est la descente de gradient, utilisé pour ajuster beaucoup de modèles de machines learning, même les modèles de deep learning.
:::

## Quelques exemples de modèles {.nonsmaller}

-   [Régression linéaire]{style="color:red"} / [Régression logistique]{style="color:blue"}

-   [Arbre de régression]{style="color:red"} / [Arbre de classification]{style="color:blue"}

-   Méthodes à base d'arbres :

    -   Forêt aléatoire

    -   Boosting

-   KNN (plus proches voisins)

-   SVM

-   Réseaux de neurones/deep learning

::: notes
Les modèles sont plus ou moins bien adaptés aux données fournies. Certains algorithmes sont par exemple plus gourmands en ressources informatiques que d'autres. Par exemple, les forêts aléatoires sont gourmands en ressources informatiques, bien davantage que les régréssions.

Le deep learning est adapté pour des données volumineuses comme les images ou les vidéos.

Certains algorithmes sont plus interprétables que d'autres pour lesquelles il y a une boîte noire qui est difficile à décrypter pour des non initiés (exemple : le deep learning).
:::

## Prédire ou estimer ?

::: panel-tabset
## Base de départ

```{r}
locations <- data.frame(Y_prix = c(1000,800,600,1000,800),X1_surface = c(50,30,25,60,35))
row.names(locations) <- c("logement1","logement2","logement3","logement4","logement5")
kbl(locations,
    format = "html")
```

## Modèle

```{r}
reg <- lm(locations$Y_prix ~ locations$X1_surface)
```

$$
Y = 416,5 + 10,6 X_1
$$

## Estimations

```{r}

locations$prix_estime <- predict(reg) %>% round(1)
kbl(locations,
    format = "html")
```

## Prévisions

```{r}
locations <- data.frame(surface = c(45,30,110))
row.names(locations) <- c("logement6","logement7","logement8")



locations <- locations %>% 
  mutate(prix_prevu = reg$coefficients[1] + reg$coefficients[2]*surface) %>% round(1)
kbl(locations,format = "html")

```
:::

::: notes
C'est une distinction essentielle en machine learning : les estimations sont les valeurs données par le modèle aux individus qui ont servi à ajuster ce modèle. C'est à partir de ces estimations qu'on construit des indicateurs pour évaluer la qualité d'un modèle. Les prévisions sont les valeurs données par le modèle à des individus qui n'ont pas servi à ajuster le modèle.

Par exemple ici, on cherche à prédire le prix d'un logement en fonction de sa surface. J'ai une base de données de 5 logements étiquetés (avec le prix) et une variable explicative (la surface). Il s'agit donc d'un problème de régression (et pas de classification). J'ajuste une régression linéaire sur mes 5 individus (j'estime les paramètres de la régression), et je calcule les estimations. A partir de cette instance de modèle, je peux faire des prévisions sur des données qui n'ont pas servi à construire le modèle (les logements 6, 7 et 8).
:::

## Comment savoir si un modèle a bien appris ?

::: columns
::: {.column width="50%"}
![](entraînement_test.png)
:::

::: {.column width="50%"}
-   Le modèle apprend (s'ajuste) sur une base d'entraînement.

-   Un modèle entraîné sur une base s'appelle une instance.

-   On évalue la performance de cette instance sur une base de validation.
:::
:::

::: notes
En machine learning, la performance d'un modèle est sa capacité à prédire sur des données qu'il n'a jamais vues. En fonction du type de problème (régression ou classification), les indicateurs d'évaluation ne seront pas les mêmes.
:::

## Problème de régression {.smaller}

::: panel-tabset
## Base Brute

```{r}
locations <- data.frame(Y_prix = c(1000,800,600,1000,800,900,300,1500),X1_surface = c(50,30,25,60,35,45,30,110))
row.names(locations) <- c("logement1","logement2","logement3","logement4","logement5","logement6","logement7","logement8")
kbl(locations,
    format = "html",caption = "locations")
```

## Base d'entraînement

```{r}
locations_train <- data.frame(Y_prix = c(1000,800,600,1000,800),X1_surface = c(50,30,25,60,35))
row.names(locations_train) <- c("logement1","logement2","logement3","logement4","logement5")
kbl(locations_train,
    format = "html",caption="location_train")
```

## Apprentissage

```{r}
reg <- lm(locations_train$Y_prix ~ locations_train$X1_surface)
summary(reg)



```

## Base de validation

```{r}
locations_valid <- locations[6:8,]

#workflow sur la base d'entraînement
rec <- recipe(Y_prix ~ ., data = locations_train) #aucune recette appliquée
mod_lr <- linear_reg() %>% 
  set_engine("lm")
wflow <- workflow() %>% 
  add_recipe(rec) %>% 
  add_model(mod_lr)

#ajustement sur la base d'entraînement
fit_lr <- wflow %>% fit(data=locations_train)

#Prédiction sur la base de validation
pred <- augment(fit_lr, locations_valid) #%>% select(-`.resid`) %>% rename(Y_REG = `.pred`)
row.names(pred) <- c("logement6","logement7","logement8")

kbl(pred,digits=c(1,1,0,0),caption = "locations_valid")
```
:::

::: notes
La base de départ est partitionnée en une base d'entraînement et une base de validation. L'algorithme s'ajuste sur la base d'entraînement (5 logements) et cette instance est utilisée pour prédire sur la base de validation (qui n'a pas servi à construire le modèle).
:::

## Indicateurs de performances

$$                                        
 RSS = \sum_{i=1}^{N}{(y_i - \hat{y}_i)^2} 
 $$

$$                                        
 MSE = \frac{RSS}{N}                     
 $$

$$                                                     
                                             RMSE = \sqrt{MSE}                                   
                                             $$

$$                                                     
                                             R² = \frac{SCE}{SCT}  
                                             $$

::: notes
Les valeurs des indicateurs de performance donnent une première idée de la qualité du modèle en matière de prédiction (on voit notamment que le coeff de détermination est très faible). Mais la vraie utilité, c'est pour comparer plusieurs modèles entre eux : on verra un exemple plus bas.\

RSS (risque quadratique) = 7,1² + 434,1² + 81,2² = 195 087

MSE (risque quadratique moyen) = 65 029

RMSE = 255

R² (coeff de détermination)= 0.18
:::

## Problème de classification {.smaller}

::: panel-tabset
## Base Brute

```{r}
canards <- data.frame(Y_femelle = c("1","1","0","1","1","0","0","0","1","0"),X1_poids = c(841,600,1200,500,700,1150,750,800,680,910)) %>% mutate(Y_femelle = as.factor(Y_femelle))
row.names(canards) <- c("canard1","canard2","canard3","canard4","canard5","canard6","canard7","canard8","canard9","canards10")
kbl(canards,
    format = "html",caption = "canards")
```

## Base d'entraînement

```{r}
canards_train <- canards[1:7,]
row.names(canards_train) <- c("canard1","canard2","canard3","canard4","canard5","canards6","canards7")
kbl(canards_train,
    format = "html",
caption="canards_train")
```

## Apprentissage

```{r}


#workflow sur la base d'entraînement
rec <- recipe(Y_femelle ~ ., data = canards_train) 

mod_lr <- 
  logistic_reg() %>% 
  set_engine("glm")
wflow <- workflow() %>% 
  add_recipe(rec) %>% 
  add_model(mod_lr)

#ajustement sur la base d'entraînement
fit_lr <- wflow %>% fit(data=canards_train)
fit_lr


```

## Base de validation

```{r}

canards_valid <- canards[8:10,]

#Prédiction sur la base de validation
pred <- augment(fit_lr, canards_valid) #%>% select(-`.resid`) %>% rename(Y_REG = `.pred`)
row.names(pred) <- c("canards8","canards9","canards10")

kbl(pred,digits=c(1,2,2,0,0),caption = "canards_valid",centering = TRUE)
```
:::

::: notes
On est sur un problème de classification puisque la variable à prédire est une qualitative, avec deux modalités (le canard est-il une femelle ?). Les données brutes sont étiquetées (ou labellisées), c'est à dire qu'on connaît Y pour les 10 canards.

On divise la base de données initiale en deux : une partie pour entraîner le modèle (ici la régression logistique) et une partie pour évaluer la performance de l'instance du modèle.

La régression logistique s'entraîne sur les 7 premiers canards pour estimer des paramètres. Et cette instance fait des prévisions sur les 3 derniers canards, qui n'ont pas servi à construire le modèle. Pour chaque canard et chaque modalité, le modèle renvoie une probabilité. si cette probabilité est supérieure à 0,5, alors la modalité prédite est celle là.
:::

## Indicateurs de performances {.smaller}

```{r}
# pred %>%
#   conf_mat(Y_sexe, .pred_class) %>%
#   autoplot(type="heatmap")
```

![](matrice_de_confusion.png){fig-align="left"}

**Exactitude** : (TN + TP) / (TN + TP + FP + FN)

**Sensibilité (Rappel, ou taux de vrais positifs)** : TP / (TP + FN)

**Spécificité** : TN / (TN + FP)

**Précision** : TP / (TP + FP)

::: notes
**Exactitude** : Mesure la capacité prédictive globale du modèle.

**Sensibilité :** Mesure la capacité du modèle à prédire les positifs.

**Spécificité** : Mesure la capacité du modèle à prédire les négatifs.

**Précision :** quand on souhaite minimiser les faux positifs.
:::

## La validation croisée

![](Validation_croisée.png){fig-align="center"}

::: notes
En pratique, on met en place un cadre de **validation croisée**. En orange, les points qui servent à l'entraînement de mon algorithme, en blanc les points qui vont servir à valider (sur lesquels je fais des prédictions). Dans cet exemple je découpe ma base en 5, et j'entraîne mon algorithme 5 fois, sur 4/5 de la base de départ. J'ai donc 5 instances qui font des prévisions successives sur 1/5 de la base.

L'intérêt est que tous les points de la base auront servi au moins une fois pour l'entraînement. Et j'aurais moins de risque d'avoir un biais d'échantillonage que dans la validation simple.
:::

## Les deux écueils de l'apprentissage

-   **Le sous-apprentissage** : le modèle n'est pas adapté ou trop simple.

-   **Le sur-apprentissage** : le modèle colle trop aux données d'apprentissage.

## A la recherche du bon modèle {.smaller}

::: {.panel-tabset .smaller}
## Sous-Apprentissage {.smaller}

![](sous_apprentissage.png){fig-align="center" width="500"}

## Bon modèle {.smaller}

![](apprentissage_adapté.png){fig-align="center" width="500"}

## Sur-apprentissage {.smaller}

![](sur-apprentissage.png){fig-align="center" width="500"}
:::

::: notes
Un modèle qui apprend correctement ne doit ni être trop simple ni trop complexe. si il est trop simple, il prédira probablement mal (puisqu'il estime mal). Si il est trop complexe, il colle trop aux données ayant servi à construire le modèle, et donc il ne va pas bien marcher avec de nouvelles données.

Le but est de prédire la couleur des points (orange ou bleu), c'est donc un problème de classification supervisée :

-   dans le premier cas, le modèle est trop simple : il va mal estimer et il va mal prédire (trop de biais)

-   Dans le deuxième cas, le modèle est trop complexe : il va bien estimer mais il risque de mal prédire

-   Dans le dernier cas, l'estimation est correcte (même si elle est moins bonne que dans le cas précédent) mais la prévision risque d'être meilleure, parce qu'elle colle moins aux données.
:::

## Le compromis biais-variance

![](biais_variance.png){fig-align="center"}

::: notes
Si un modèle est trop simple (comme ici la droite qui modélise mal le nuage de points), il fera beaucoup d'erreurs d'estimations ET de prédictions sur de nouvelles données.

Plus on complexifie ce modèle, moins il fait d'erreurs d'estimations. Pour un modèle de régression, un modèle se complexifie quand on lui rajoute des variables. On verra que dans le cas d'un arbre de décision, plus un arbre est profond (plus il a de feuilles), moins il fait d'erreurs d'estimation. Quand on arrive à un seuil trop important de complexité, on dit que le modèle colle trop aux données d'entraînement. Autrement dit, il surraprend, et il risque de ne pas bien s'adapter à un nouveau jeu de données, qui n'aura pas tout à fait la même structure.

Il faut donc réussir à trouver un modèle qui ne sous-apprend pas et qui ne tombe pas non plus dans le piège du surraprentissage.
:::

## Autre exemple

![](sur_sous_apprentissage.PNG){fig-align="left"}

## Un arbre de décision, c'est quoi ?

![](Exemple-darbre-de-regression.png){.smaller fig-align="center" width="400"}

::: notes
Voilà un exemple d'arbre de décision, qui a pour put de prédire la taille en fonction du sexe, de l'age et du fait d'habiter en ville. Ici on a un arbre de décision qui prédit que les hommes de moins de 28 ans habitant en ville mesurent 1,80 m.

Un arbre est composé de noeuds (split en anglais) et de feuilles. Le premier noeud est appelé la racine. C'est donc un arbre renversé. Les noeuds sont segmentés à l'aide d'une variable qui peut être quantitative ou qualitative.
:::

## Comment est-il construit ? {.smaller .scrollable}

L'algorithme CART permet d'ajuster le modèle sur les données d'entraînement.

1.  L'utilisateur paramètre la profondeur de l'arbre.

2.  A chaque noeud, l'algorithme (CART) choisit la variable la plus discriminante.

3.  Il teste toutes les valeurs de cette variable et choisit celle qui discrimine le mieux.

4.  Le noeud est divisé en deux parties qui deviennent soit des feuilles soit des noeuds.

5.  L'estimation est soit la moyenne (pour la régression) soit la modalité majoritaire (pour la classification).

::: notes
CART : classifications and regression trees. C'est l'algorithme le plus utilisé, il est adapté à régression et la classification. Il en existe d'autres. On ne peut pas trop rentrer dans le détail du fonctionnement de l'algorithme (notamment comment choisit-il la variable la plus discriminante et à quel moment il décide de s'arrêter pour constituer une feuille.)

Pour tout savoir sur les arbres de régression (et de classification) :

<https://blent.ai/blog/a/arbres-de-decision-en-machine-learning>
:::

## La forêt aléatoire : mieux qu'un arbre ! {.smaller}

-   L'arbre de décision est un apprenant faible (faibles performances)...

-   La forêt aléatoire entraîne un grand nombre instances d'arbres :

    -   sur des données légèrement différentes (bootstrap)

    -   sur une partie des variables explicatives

-   Les estimations de tous les arbres sont ensuite combinées :

    -   pour la régression : on prend la moyenne

    -   pour la classification : on prend la valeur majoritaire

::: notes
Les performances des arbres sont bien meilleures parce qu'il est plus robuste. Le fait d'entraîner un grand nombre d'arbres sur des données légèrement différentes et avec différentes variables permet d'améliorer ses performances prédictives.

L'utilisateur continue de paramétrer la profondeur de l'arbre + le nombre d'arbres à entraîner + le nombre de variable pour chaque arbre.
:::

## Forêt aléatoire : illustration

![](foret_aléatoire.png){fig-align="center" width="780"}

.

::: notes
on voit que les arbres sont différents et ne prédit pas tous la même chose. A la fin on prend soit la moyenne (pour la régression) soit un vote à la majorité (pour la classif).
:::

## Le KNN : les plus proches voisins

::: columns
::: {.column width="50%"}
![Classification supervisée](KNN_classif.png){fig-align="left"}
:::

::: {.column width="50%"}
![Régression supervisée](KNN_regression.png){width="398"}
:::
:::

::: notes
-   L'utilisateur commence par paramérer le nombre de voisins (K)

-   L'algorithme calcule les distances de tous les individus entre eux, et pour chaque individu, il trouve les K plus proches.

-   En classification supervisée, l'algorithme attribue la classe majoritaire (les chats pour l'image de gauche).

-   En régression, l'algorithme prend la moyenne des K individus les proches (K=3 pour l'image de droite).
:::

## Les étapes de l'apprentissage supervisé : {.smaller}

1.  Définir le sujet (classification ou régression ?)

2.  Explorer et nettoyer la base de données

3.  Réserver une partie de la base pour l'apprentissage/validation/test

4.  Apprendre des données avec des modèles/algorithmes

5.  Comparer les performances prédictives de ces algorithmes

6.  Choisir le meilleur modèle et le mettre en production

# Premier exemple

## Régression ou classification ? {.scrollable .smaller}

```{r}
grandile <- asta::grandile %>% 
select(Y_REVENU = REV_DISPONIBLE, X1_NBPIECES= NBPIECES,X2_AGE = AGE)
kbl(head(grandile,100),caption="Grandile")
```

::: notes
Il s'agit d'un problème de régression puisque la variable à expliquer est quantitative continue. Les variables explicatives sont deux quantitatives (dont une est discrète).
:::

## Exploration de la base brute {.scrollable .smaller}

```{r}
# summary(grandile)
# print("Il y a",nrow(grandile),"ménages dans le fichier Grandile")
skim(grandile)
```

::: notes
On voit qu'il n'y a pas de valeurs manquantes, et on a les ordres de grandeurs des variables. On a aussi les distributions.

Pour aller plus loin dans l'exploration, on pourrait explorer les liens linéaires entre les variables prises deux à deux (quanti-quanti, voire quali-quanti si on considère que le nombre de pièces est une variable discrète).
:::

## Nettoyage et transformation des données

-   Traitement des données manquantes

-   Traitement des "outliers"

-   Encodage de variables

-   Transformation de variables (log, centrage-réduction)

-   Création de nouvelles variables

## Partition de la base

![](training-validation-test-sets.png){fig-align="center"}

-   Base d'entraînement : 60 %

-   Base de validation : 20 %

-   Base de test : 20 %

::: notes
Dans cet exemple, les bases sont tirées au hasard. Mais il est possible de faire en sorte d'avoir des bases représentatives (pour avoir un poids suffisant de certaines modalités de variables). Par exemple, si on a vraiment pas de chances, on pourrait n'avoir aucun "riche" dans notre base d'entraînement, et que des riches dans la base de validation. Notre modèle ne prédirait pas bien sur la base de validation.

C'est la raison pour laquelle on préfère toujours faire de la **validation croisée** (avec un découpage en 5 ou 10). Mais il faut avoir conscience qu'elle prend de la ressource informatique.
:::

## Base d'entraînement {.scrollable .smaller}

```{r}
part_training <- 0.6 
part_validation <- 0.2
set.seed(1234)

grandile_split <- initial_validation_split(grandile,prop = c(part_training,part_validation))

train_grandile <- training(grandile_split) #le fichier d'entraînement
test_grandile <- testing(grandile_split) #le fichier de test
valid_grandile <- validation(grandile_split)
train_valid_grandile <- train_grandile %>% bind_rows(valid_grandile)
```

```{r}
skim(train_grandile)
# summary(train_grandile)
```

::: notes
La base d'entraînement n'est pas toujours la même puisqu'elle a été tiré au hasard. Dans cet exemple, on a bloqué l'aléatoire (set.seed).
:::

```{r}
#Les recettes
rec1 <- 
  recipe(Y_REVENU ~ ., data = train_grandile) 
```

```{r}

#les modèles

#modèle de regression linéaire
mod_lr <- linear_reg() %>% 
  set_engine("lm")

mod_tree <- 
  mod_tree <- 
  decision_tree(
    cost_complexity = 0.02,
    # tree_depth = 7,
    # min_n = NULL
  ) %>%
  set_engine("rpart") %>%
  set_mode("regression")

```

## Modèle 1 : Régression linéaire

```{r}
wflow <- workflow() %>% 
  add_recipe(rec1) %>% 
  add_model(mod_lr)

fit_lr <- wflow %>% fit(data=train_grandile)
fit_lr


```

::: notes
C'est le résultat de l'ajustement de la régression linéaire sur la base d'entraînement : on a donc trois coefficients, qu'on utilisera pour prédire.

On peut déjà avoir des indicateurs de qualité du modèle avec les estimations (cf module de stat sur la modélisation).
:::

## Modèle 2 : Arbre de régression (visualisation) {.smaller}

```{r}
arbre1 <- rpart(Y_REVENU ~ .,data=train_grandile,cp=0.02) 
rpart.plot(arbre1)
```

::: notes
Les cercles représentent une partie de la population d'étude. Le première cercle est appelé la racine,.
:::

## Modèle 2 : sortie R

```{r}
wflow <- workflow() %>% 
  add_recipe(rec1) %>% 
  add_model(mod_tree)

fit_tree <- wflow %>% fit(data=train_grandile)
fit_tree
# summary(arbre1)

```

## Prévisions sur la base de validation {.smaller .scrollable}

```{r}
pred_modele1 <- augment(fit_lr, valid_grandile) %>% select(-`.resid`) %>% rename(Y_REG = `.pred`)
pred_modele2 <- augment(fit_tree,valid_grandile) %>% rename(Y_ARBRE = `.pred`) %>% select(Y_ARBRE) 
pred <- pred_modele2 %>% cbind(pred_modele1) %>% round() %>% select(Y_REVENU,X1_NBPIECES,X2_AGE,Y_REG,Y_ARBRE)
kbl(pred)
```

## Comparaison des performances

|        [Modèle]{.underline}        |                         [MSE]{.underline}                         |                           [RMSE]{.underline}                            |                                 [R²]{.underline}                                  |
|:---------------:|:---------------:|:---------------:|:------------------:|
| **Modèle 1 : Régression linéaire** |  `r sum((pred$Y_REVENU - pred$Y_REG)^2)/nrow(pred) %>% round(1)`  |  `r sqrt(sum((pred$Y_REVENU - pred$Y_REG)^2)/nrow(pred)) %>% round(1)`  |  `r rsq(pred,Y_REVENU,Y_REG) %>% select(.estimate) %>% as.numeric()%>% round(2)`  |
|        **Modèle 2 : Arbre**        | `r sum((pred$Y_REVENU - pred$Y_ARBRE)^2)/nrow(pred) %>% round(1)` | `r sqrt(sum((pred$Y_REVENU - pred$Y_ARBRE)^2)/nrow(pred)) %>% round(1)` | `r rsq(pred,Y_REVENU,Y_ARBRE) %>% select(.estimate) %>% as.numeric()%>% round(2)` |

## Optimisation des modèles

1.  Et si on retirait une variable ? ajoutait une variable ?

2.  Et si on essayait avec un arbre plus profond ? (hyper-paramètres)

3.  Et si on construisait des variables à partir des variables existantes ? (X², Log(X) etc...)

::: notes
On ne le fait pas dans cet exemple, mais en pratique, il faudrait tester les performances de ces "nouveaux" modèles, pour voir si ils prédisent mieux que ceux-là. Peut-être que la régression sur une variable prédit mieux que la régression sur deux variables. Peut-être que notre arbre n'est pas suffisament profond (hyper-paramètre qu'on a laissé par défaut).

En pratique, il faudrait tester un grand nombre d'arbre et prendre celui avec la profondeur qui prédit le mieux (sur des données qu'elle n'a jamais vues). Des fonctions R permettent de le faire.
:::

## Entraînement du modèle final

```{r}
fit_tree_final <- wflow %>% fit(data=train_valid_grandile)
fit_tree_final
```

::: notes
Après avoir testé le pouvoir prédictif d'un nombre suffisant de modèles, sur des données transformées ou non, on retient un et un seul modèle qui sera mis en production pour faire des prédictions. On ne reviendra pas sur notre choix.

Dans notre cas, on a retenu un arbre de régression, avec la profondeur d'arbre par défaut. Il faut donc ajuster cet arbre sur les données d'entraînement et les données de validation, pour avoir un maximum de données.
:::

## Généralisation/Mise en production {.scrollable .smaller}

```{r}
pred_modele_final <- augment(fit_tree_final, test_grandile) %>% select(-`.resid`,-Y_REVENU) %>% rename(Y_PREVU = `.pred`) %>% round()
kbl(head(pred_modele_final,100))
```

::: notes
Voilà les prédictions obtenues à partir de notre base de test, qui a l'avantage de ne pas avoir du tout servi pour sélectionner notre modèle. En théorie, on pourrait construire des indicateurs de qualité de prédiction. Mais elles ne remettront pas en cause le choix de notre modèle, qui est celui retenu pour la mise en production.

Le modèle n'est pas top, on pourrait faire bien mieux que ça, mais avec d'autres variables explicatives !
:::

# Deuxième exemple

## Régression ou classification ? {.scrollable .smaller}

```{r}

grandile <- asta::grandile %>%
select(Y_PAUVRE = PAUVRE, X1_REVENU = REV_DISPONIBLE, X2_DIPL = DIPL) %>% 
mutate(Y_PAUVRE = as.factor(Y_PAUVRE), X2_DIPL = as.factor(X2_DIPL))

kbl(head(grandile,100))

```

::: notes
Il s'agit d'un problème de classification. On cherche à prédire la variable Y_PAUVRE, qui est une qualitative à 2 modalités, à partir d'une variable quantitative (le revenu) et d'une qualitative (le niveau de diplôme, qualitative ordinale).
:::

## Exploration de la base brute {.scrollable .smaller}

```{r}
# summary(grandile)
skim(grandile)
```

::: notes
La proportion de pauvres est bien inférieure à celle des non pauvres. Il faudra en tenir compte dans l'étape suivante. Pour s'entraîner, l'algorithme aura besoin de suffisamment d'individus de chaque modalité.
:::

## Partition

![](training-validation-test-sets.png){fig-align="center"}

-   Base d'entraînement : 60 %

-   Base de validation : 20 %

-   Base de test : 20 %

::: notes
La partition n'est pas complètement aléatoire, puisqu'on cherche à avoir une proportion de pauvres identique dans chacune des bases.
:::

## Exploration de la base d'entraînement {.scrollable .smaller}

```{r}
part_training <- 0.6 
part_validation <- 0.2
set.seed(1234)

grandile_split <- initial_validation_split(grandile,
                                            prop = c(part_training,part_validation),
                                            strata = Y_PAUVRE)

train_grandile <- training(grandile_split) #le fichier d'entraînement
test_grandile <- testing(grandile_split) #le fichier de test
valid_grandile <- validation(grandile_split)
train_valid_grandile <- train_grandile %>% bind_rows(valid_grandile)
```

```{r}
# summary(train_grandile)
# nrow(train_grandile)
skim(train_grandile)
```

```{r}
rec1 <- 
  recipe(Y_PAUVRE ~ ., data = train_grandile) 
# %>% step_normalize(all_numeric_predictors()) 
# %>% step_dummy(all_nominal_predictors()) %>% #: pour transformer les variables nominales en indicatrices
# %>% update_role(flight, time_hour, new_role = "ID") %>% #: pour retirer des variables du modèle
# %>%step_normalize(all_numeric_predictors()) #pour centrer réduire
# %>% step_zv() #pour enlever les variables avec une seules valeur
# %>% step_rm() #removes variables
# %>% step_impute_mode() #imputation des valeurs manquantes avec le mode
# %>% step_impute_mean() #imputation des valeurs manquates avec la moyenne
# %>% step_clean_names #nettoyer le nom des variables
```

::: notes
On vérifie que la base d'entraînement a bien la même proportion de pauvres que la base brute.
:::

## Modèles à entraîner

```{r}
mod_lr <- 
  logistic_reg() %>% 
  set_engine("glm")

mod_rf <- 
  rand_forest(trees = 1000,
              mtry = 3,
              min_n = NULL) %>% 
  set_engine("ranger") %>% 
  set_mode("classification")


mod_tree <- 
  mod_tree <- 
  decision_tree(
    cost_complexity = 0.02,
    tree_depth = 7,
    min_n = NULL
  ) %>%
  set_engine("rpart") %>%
  set_mode("classification")
```

1.  Régression logistique

2.  Arbre de classification

3.  Forêt aléatoire

::: notes
On ajoute la forêt aléatoire par rapport à l'exemple précédent. En général, c'est l'algorithme qui a les meilleures performances, mais il faut le vérifier. C'est un algorithme gourmand en ressources.
:::

## Modèle 1 : régression logistique

```{r}
wflow <- workflow() %>% 
  add_recipe(rec1) %>% 
  add_model(mod_lr)

fit_lr <- wflow %>% fit(data=train_grandile)
fit_lr
```

::: notes
Le modèle s'ajuste sur les données d'entraînement, et on récupère les coefficients.
:::

## Prévisions (modèle 1) {.scrollable}

```{r}
pred_valid <- augment(fit_lr, valid_grandile) %>% 
select(Y_PAUVRE,starts_with(".pred")) %>% 
mutate_if(is.numeric,round,digits=2) %>% 
rename(`Y_REG`=`.pred_class`)
kbl(head(pred_valid,100))
```

::: notes
On établit des prévisions sur la base de validation à partir des probabilités estimées par le modèle. Si la probabilité associée à une modalité est supérieure à 0,5, alors le modèle renvoie cette modalité.

A partir de ces prévisions sur la base de validation, on va pouvoir classer les individus dans une table de confusion.
:::

## Table de confusion (modèle 1)

```{r}
pred_valid %>%
  conf_mat(Y_PAUVRE, Y_REG) %>%
  autoplot(type="heatmap")
```

::: notes
C'est à partir de cette table de confusion qu'on a va calculer les indicateurs de performances.
:::

## Performances (modèle 1)

```{r}
#Accuracy : pourcentage de biens classés
accuracy_reg <- pred_valid %>% 
  accuracy(truth = Y_PAUVRE, Y_REG) %>% as.numeric() %>% round(2)

#la fonction specificity = sensibilité (recall)
spec_reg <- pred_valid %>% 
  specificity(truth = Y_PAUVRE, Y_REG) %>% as.numeric() %>% round(2)

#sensitivity = spécificité (capacité à prévoir du négatif)
sens_reg <- pred_valid %>% 
  sensitivity(truth = Y_PAUVRE, Y_REG) %>% as.numeric() %>% round(2)
```

|                                  | Exactitude          | Sensibilité     | Spécificité     |
|-----------------------|-----------------|-----------------|-----------------|
| Modèle 1 : régression logistique | `r accuracy_reg[3]` | `r spec_reg[3]` | `r sens_reg[3]` |

::: notes
Le modèle classe mal les positifs (c'est à dire les pauvres). Il s'est trompé dans 60% des cas puisque la sensibilité (ou recall, rappel) est égale à 0,4. Donc il faut se méfier quand il prédit qu'un ménage est pauvre. Si on veut qu'un modèle soit meilleur pour prédire les positifs, une solution consiste à changer le seuil, passer à 0,7 par exemple.
:::

## Modèle 2 : arbre de classification

```{r}
wflow <- workflow() %>% 
  add_recipe(rec1) %>% 
  add_model(mod_tree)

fit_tree <- wflow %>% fit(data=train_grandile)
fit_tree

```

```{r}
pred_tree <- augment(fit_tree, valid_grandile) %>% 
select(`.pred_class`) %>% 
mutate_if(is.numeric,round,digits=2) %>% 
rename(`Y_ARBRE`=`.pred_class`)

```

## Modèle 3 : forêt aléatoire

```{r}
wflow <- workflow() %>% 
  add_recipe(rec1) %>% 
  add_model(mod_rf)

fit_rf <- wflow %>% fit(data=train_grandile)
fit_rf
```

```{r}
pred_rf <- augment(fit_rf, valid_grandile) %>% 
select(`.pred_class`) %>% 
mutate_if(is.numeric,round,digits=2) %>% 
rename(`Y_FORET`=`.pred_class`)

```

## Comparaison des modèles

```{r}
pred_valid <- pred_valid %>% bind_cols(pred_tree) %>% bind_cols(pred_rf) %>% select(starts_with("Y"))

#indicateurs pour les arbres

accuracy_tree <- pred_valid %>% 
  accuracy(truth = Y_PAUVRE, Y_ARBRE) %>% as.numeric() %>% round(2)
spec_tree <- pred_valid %>% 
  specificity(truth = Y_PAUVRE, Y_ARBRE) %>% as.numeric() %>% round(2)
sens_tree <- pred_valid %>% 
  sensitivity(truth = Y_PAUVRE, Y_ARBRE) %>% as.numeric() %>% round(2)

#Indicateurs pour les forêts
accuracy_rf <- pred_valid %>% 
  accuracy(truth = Y_PAUVRE, Y_FORET) %>% as.numeric() %>% round(2)
spec_rf <- pred_valid %>% 
  specificity(truth = Y_PAUVRE, Y_FORET) %>% as.numeric() %>% round(2)
sens_rf <- pred_valid %>% 
  sensitivity(truth = Y_PAUVRE, Y_FORET) %>% as.numeric() %>% round(2)

```

|                                    | Exactitude           | Spécificité      | Sensibilité      |
|-----------------------|-----------------|-----------------|-----------------|
| Modèle 1 : régression logistique   | `r accuracy_reg[3]`  | `r spec_reg[3]`  | `r sens_reg[3]`  |
| Modèle 2 : arbre de classification | `r accuracy_tree[3]` | `r spec_tree[3]` | `r sens_tree[3]` |
| Modèle 3 : forêt aléatoire         | `r accuracy_rf[3]`   | `r spec_rf[3]`   | `r sens_rf[3]`   |

## Optimisation

1.  Et si on ajoutait/retirait des variables explicatives ?

2.  Et si on regroupait des modalités sur DIPL ?

3.  Et si on modifiait la profondeur de l'arbre (modèle 2) ?

4.  Et si modifiait le nombre d'arbres tirés aléatoirement ? Le nombre de variables ? (modèle 3)

## Entraînement du modèle final

```{r}
wflow_final <- workflow() %>% 
  add_recipe(rec1) %>% 
  add_model(mod_rf)

fit_final <- wflow_final %>% fit(data=train_valid_grandile)
fit_final
```

## Généralisation/Mise en production {.smaller .scrollable}

```{r}
pred_final <- augment(fit_final, test_grandile) %>% 
select(Y_PAUVRE,starts_with(".pred"),X1_REVENU,X2_DIPL) %>% 
mutate_if(is.numeric,round,digits=2) %>% 
rename(`Y_PREDICT`=`.pred_class`)
kbl(head(pred_final,100))
```

## A vous de jouer maintenant !

![](JO2024.jpg){fig-align="center"}
