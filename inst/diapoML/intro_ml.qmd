---
title: "Introduction au machine learning"
format: 
  revealjs:
    incremental: true
    embed-resources: true #pour tout mettre dans un seul fichier
editor: visual
---

```{r}
library(kableExtra)
library(dplyr)
library(asta) #pour récupérer la base titanic
library(skimr)
library(tidymodels)
library(rpart.plot)
```

# Les grands principes

## Vous avez dit "machine learning" ?

![](IA_ML.png){fig-align="center"}

::: notes
L'intelligence artificielle définie par Yann le Cun [@Cun2019] : *l'intelligence artificielle est la capacité pour une machine d'assurer des tâches généralement assurée par les animaux ou les humains : percevoir, raisonner, agir.*

Le machine learning fait partie d'intelligence artificielle, mais il se distingue de l'IA historique par son mode de fonctionnement.

Le deep learning est un machine learning très puissant, utilisé sur des données très volumineuses (images, vidéos..) et nécessitant de grosses puissances de calculs.
:::

## "La bonne vieille IA"...

![](système_expert.png){fig-align="center"}

::: notes
Il y a "la bonne vieille intelligence artificielle" en français, celle pour laquelle un expert explicite toutes les règles qui sont ensuite traduites en langue informatique. Il faut l'expert à côté de l'informaticien qui code, pour lui donner toutes les règles.

C'est celle qu'on utilise depuis longtemps, depuis les années 50.
:::

## ...VS l'apprentissage automatique

![](machine-learning-entrainement-ia.jpg){fig-align="center"}

::: notes
Avec le machine learning, on donne à la machine une grande quantité de donnée, et elle va trouver 'toute seule' cette règle, c'est pour ça qu'on dit qu'elle apprend. Elle apprend grace à un algorithme qui permet de traiter une (très) grande quantité de donnée pour en faire ressortir un schéma réccurent.
:::

<!-- ## et le Deep learning -->

<!-- ![](reseau_neurones.jpg){fig-align="center"} -->

<!-- :::notes -->

<!-- Le deep learning est un sous domaine du machine learning : globalement, il veut reproduire le cerveau humain grace à des neurones articifiels. très puissant en reconnaissance d'images par exemple. -->

<!-- ::: -->

## Les différents types d'apprentissage automatique

1.  L'apprentissage supervisé

2.  L'apprentissage non supervisé

3.  L'apprentissage par essai/erreur

::: notes
Dans ce module d'introduction, on va principalement étudier l'apprentissage supervisé. L'apprentissage non supervisé a été vu en partie dans le module sur la modélisation. Enfin, le dernier type d'apprentissage ne sera pas vu, c'est celui qui est utilisé pour simuler un joueur de jeu de go ou d'échec par exemple.

la différence entre supervisé et non supervisé est dans la diapo suivante.
:::

## Différence entre supervisé et non supervisé

![](supervisé_nonsupervisé.png){fig-align="center"}

::: notes
Dans le non supervisé, les données ne sont pas étiquetés. Le but pour la machine est de détecter les ressemblances entre les individus (cf module sur le clustering).

Dans le supervisé, la machine apprend sur des données étiquetés (avec un Y). L'image a un nom, et le but est de faire en sorte que le modèle soit capable de prédire correctement une nouvelle image.

Cette image vient du site suivant :

<https://www.le-cortex.com/media/articles/lintelligence-artificielle-comment-ca-marche>
:::

## But de l'apprentissage supervisé : la prédiction

::: columns
::: {.column width="50%"}
-   $$
    Y = f(X_1, X_2, ...,X_n)
    $$

-   Le but premier de l'apprentissage supervisé n'est pas d'expliquer mais bien de prédire.
:::

::: {.column width="50%"}
![](42293349-photo-de-future-femme-prédiction-de-la-boule-de-cristal.jpg)
:::
:::

::: notes
-   Il s'agit de trouver la fonction f qui va le mieux **prédire** Y en fonction de X1, X2...,Xn

-   On distingue la modélisation statistique (cf module 3 de statistique), avec des tests et des hypothèses. Le but est d'expliquer, d'interpréter, de comprendre les données du machine learning, qui a pour objectif d'avoir les meilleures performances prédictives, au détriment parfois de l'interprétabilité.
:::

## Régression et classification

![](types_apprentissage.png){fig-align="center"}

::: notes
Attention : en machine learning, dans un problème de régression (Y est quanti continu), je n'utilise pas nécessairement la régression linéaire (qui est un modèle parmi d'autres) comme modèle. Dans un problème de classification (Y est quali), je peux utiliser une régression logistique comme modèle pour apprendre.
:::

## Prédire ou estimer ?

::: panel-tabset
## Base de départ

```{r}
locations <- data.frame(Y_prix = c(1000,800,600,1000,800),X1_surface = c(50,30,25,60,35))
row.names(locations) <- c("logement1","logement2","logement3","logement4","logement5")
kbl(locations,
    format = "html")
```

## Modèle

```{r}
reg <- lm(locations$Y_prix ~ locations$X1_surface)
```

$$
Y = 416,5 + 10,6 X_1
$$

## Estimations

```{r}

locations$prix_estime <- predict(reg) %>% round(1)
kbl(locations,
    format = "html")
```

## Prévisions

```{r}
locations <- data.frame(surface = c(45,30,110))
row.names(locations) <- c("logement6","logement7","logement8")



locations <- locations %>% 
  mutate(prix_prevu = reg$coefficients[1] + reg$coefficients[2]*surface) %>% round(1)
kbl(locations,format = "html")

```
:::

::: notes
On cherche à prédire le prix d'un logement en fonction de sa surface. J'ai une base de données de 5 logements étiquetés (avec le prix) et une variable explicative (la surface). Il s'agit donc d'un problème de régression (et pas de classification).
:::

## Comment la machine apprend ?

-   La machine learning apprend sur des données étiquetées grace à des algorithmes (exemple : la régression linéaire pour des don).

-   Le but de ces algorithmes est de minimiser [l'erreur d'estimation]{.underline}.

-   Il existe de nombreux modèles/algorithmes.

::: notes
Un algorithme est une série d'instructions définies et ordonnées qui permettent d'effectuer une tâche spécifique. Il existe plusieurs façons d'évaluer l'erreur d'estimation, certaines ont déjà été vues pendant le TD sur les régressions. L'erreur quadratique est un exemple d'erreur d'estimation que l'algorithme de la régression cherche à estimer.
:::

## Les classes de modèles

1.  Les modèles de régression

2.  Les modèles à base d'arbre

3.  Les réseaux de neurones et le deep learning

## C'est quoi un "bon" apprentissage ?

::: panel-tabset
## Sous-Apprentissage

![](sous_apprentissage.png){fig-align="center" width="500"}

## Sur-apprentissage

![](sur-apprentissage.png){fig-align="center" width="500"}

## Bon modèle

![](apprentissage_adapté.png){fig-align="center" width="500"}
:::

::: notes
Un modèle qui apprend correctement ne doit ni être trop simple ni trop complexe. si il est trop simple, il prédira très probablement mal (puisqu'il estime mal). Si il est trop complexe, il colle trop aux données ayant servi à construire le modèle, et donc il ne va pas bien marcher avec de nouvelles données.

Le but est de prédire la couleur des points (orange ou bleu), c'est donc un problème de classification supervisée :

-   dans le premier cas, le modèle est trop simple : il va mal estimer et il va mal prédire (trop de biais)

-   Dans le deuxième cas, le modèle est trop complexe : il va bien estimer mais il risque de mal prédire

-   Dans le dernier cas, l'estimation est correcte (même si elle est moins bonne que dans le cas précédent) mais la prévision risque d'être meilleure, parce qu'elle colle moins aux données.
:::

## Le compromis biais-variance

![](biais_variance.png){fig-align="center"}

## Autre exemple

![](sur_sous_apprentissage.PNG){fig-align="left"}

# Les grandes étapes

## Les étapes de l'apprentissage supervisé :

1.  Définir le sujet (classification ou régression ?)

2.  Explorer et nettoyer la base de données

3.  Réserver une partie de la base pour l'apprentissage/test

4.  Apprendre des données avec des modèles/algorithmes

5.  Comparer les performances prédictives de ces algorithmes

6.  Choisir le meilleur modèle et le mettre en production

# Etape 1 : définir le problème

## Régression ou classification ?

```{r}
grandile <- asta::grandile %>% 
select(Y_REVENU = REV_DISPONIBLE, X1_NBPIECES= NBPIECES,X2_AGE = AGE)
kbl(head(grandile))
```

<!-- ## Régression ou classification ? -->

<!-- ```{r} -->

<!-- grandile_classif <- asta::grandile %>%  -->

<!-- select(Y_PAUVRE = PAUVRE, X1_REVENU = REV_DISPONIBLE, X2_DIPL = DIPL) -->

<!-- kbl(head(grandile_classif)) -->

<!-- ``` -->

# Etape 2 : Explorer, nettoyer et améliorer la base

## Exploration {.scrollable}

```{r}
# summary(grandile)
skim(grandile)
```

## Nettoyage et transformation des données

-   Traitement des données manquantes

-   Traitement des "outliers"

-   Encodage de variables

-   Création de nouvelles variables

# Etape 3 : Partitionner la base

## Partition de la base

![](training-validation-test-sets.png){fig-align="center"}

## Partition de grandile {.scrollable}

```{r}
part_training <- 0.6 
part_validation <- 0.2
set.seed(1234)

grandile_split <- initial_validation_split(grandile,prop = c(part_training,part_validation))

train_grandile <- training(grandile_split) #le fichier d'entraînement
test_grandile <- testing(grandile_split) #le fichier de test
valid_grandile <- validation(grandile_split)
train_valid_grandile <- train_grandile %>% bind_rows(valid_grandile)
```

```{r}
skim(train_grandile)
```

# Etape 4 : Entraînement des modèles

```{r}
#Les recettes
rec1 <- 
  recipe(Y_REVENU ~ ., data = train_grandile) 
```

```{r}

#les modèles

#modèle de regression linéaire
mod_lr <- linear_reg() %>% 
  set_engine("lm")

mod_tree <- 
  mod_tree <- 
  decision_tree(
    cost_complexity = 0.02,
    # tree_depth = 7,
    # min_n = NULL
  ) %>%
  set_engine("rpart") %>%
  set_mode("regression")

```

## Modèle 1 : Régression linéaire

```{r}
wflow <- workflow() %>% 
  add_recipe(rec1) %>% 
  add_model(mod_lr)

fit_lr <- wflow %>% fit(data=train_grandile)
summary(fit_lr)


```

## Modèle 2 : Arbre de régression, comment ça marche ?

1.  L'algorithme (CART) choisit la variable la plus discriminante

2.  Test de toutes les valeurs pour déterminer le seuil optimal

3.  Partage de la population en deux noeuds

4.  Il renouvelle les 3 premières étapes pour chaque noeud

5.  L'utilisateur définit le nombre de noeuds souhaités (max = N)

6.  Les valeurs estimées sont les moyennes des noeuds terminaux

## Modèle 2 : Arbre de regression (visualisation) {.smaller}

```{r}
arbre1 <- rpart(Y_REVENU ~ .,data=train_grandile,cp=0.02)
rpart.plot(arbre1)
```

## Modèle 2 : sortie R

```{r}
wflow <- workflow() %>% 
  add_recipe(rec1) %>% 
  add_model(mod_tree)

fit_tree <- wflow %>% fit(data=train_grandile)
fit_tree

```

<!-- ## Apprentissage avec la regression linéaire -->

<!-- ::: columns -->

<!-- ::: {.column width="60%"} -->

<!-- ```{r} -->

<!-- locations <- data.frame(Y_prix = c(1000,800,600,1000,800),X_surface = c(50,30,25,60,35)) -->

<!-- row.names(locations) <- c("logement1","logement2","logement3","logement4","logement5") -->

<!-- reg <- lm(locations$Y_prix ~ locations$X_surface) -->

<!-- kbl(locations, -->

<!--     format = "html") -->

<!-- ``` -->

<!-- ::: -->

<!-- ::: {.column width="40%"} -->

<!-- Par MCO, on trouve la fonction f : -->

<!-- $$ -->

<!-- Y = 416,5 + 10,6 X -->

<!-- $$ -->

<!-- ::: -->

<!-- ::: -->

<!-- ::: notes -->

<!-- Exemple : sur cette base avec une seule variable explicative, par la méthode des MCO (méthode des moindres carrés ordinaires), on peut trouver une droite qui passe au plus près des points du nuage de point (cf module sur les régressions linéaires). -->

<!-- ::: -->

<!-- ## Non supervisé VS supervisé -->

<!-- ::: panel-tabset -->

<!-- ## Non supervisé -->

<!-- ```{r} -->

<!-- locations <- data.frame( -->

<!--                         X1_surface = c(50,30,25,60,35), -->

<!--                         X2_pieces = c(3,2,1,4,1)) -->

<!-- kbl(locations, -->

<!--     format = "html") -->

<!-- ``` -->

<!-- ## Supervisé -->

<!-- ```{r} -->

<!-- locations <- data.frame(Y_prix = c(1000,800,600,1000,800), -->

<!--                         X1_surface = c(50,30,25,60,35), -->

<!--                         X2_pieces = c(3,2,1,4,1)) -->

<!-- kbl(locations, -->

<!--     format = "html") -->

<!-- ``` -->

<!-- ::: -->

<!-- ::: notes -->

<!-- En apprentissage non supervisée, les données ne sont pas étiquetées. Le but est de regrouper automatiquement les individus qui se ressemblent, et de leur trouver ensuite un nom qui les rassemblent : par exemple les grands logements vont être ensemble. Permet de faire de la classification automatique. En apprentissage supervisé, j'ai des données étiquetées (avec un Y) et le but est d'être capable de prédire Y à partir de ces données étiquetées. -->

<!-- ::: -->
