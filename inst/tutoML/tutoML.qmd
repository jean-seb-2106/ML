---
title: "Tuto Machine learning sous R"
author: "Cefil-2024"
date: "27/06/2024"
format: 
  html:
    toc: true
    toc-depth: 4
    number-sections: true
    code-fold: false
    df-print: paged
editor: visual
embed-resources: true
---

En complément du corrigé des exercices, voici le code R sous-jacent à Asta, afin que vous puissiez produire ou appliquer les méthodes de machine learning directement sous R.

C'est le package `tidymodels` qui est utilisé dans ASTA pour faire du machine learning. Il regroupe et intègre les packages r suivants :`rsample`, `recipes`, `tune`, `parnsnip`, `yardstick`... Chacun de ces packages étant utile pour le machine learning. Par ailleurs, on utilise le package `skimr` pour explorer les fichiers.

Les données de l'exercice sont intégrées au package ASTA. On distingue les deux grands types d'apprentissage automatique :

-   la classification supervisée

-   la régression supervisée.

::: callout-warning
L'application ASTA ne permet pas de paramétrer les différents modèles : ces paramètres sont définis par défaut dans l'application. Elle ne permet pas non plus de faire de la validation croisée (qui est d'usage en machine learning) ni d'optimiser chaque modèle pour trouver les meilleurs hyper-paramètres. Ces points ne seront donc pas abordés dans ce tutoriel, mais il est possible de faire ça avec `tidymodels`.
:::

```{r}
#| message: false
#| warning: false
# install.packages("tidymodels")
# install.packages("skimr")
#install.packages("glmnet")
library(tidymodels)
library(skimr)
library(glmnet)
```

## Classification supervisée

Les deux fichiers utilisés pour faire de la classification dans `asta` sont le fichier `vin` (le vin est-il bon ou mauvais ?) et le fichier `grandile` (les ménages sont-ils pauvres ?). On fait quelques modifications sur ces fichiers pour faire en sorte que la variable à expliquer ait le même nom dans les deux fichiers (`target`).

```{r}
#| message: false
#| warning: false
library(asta)
data <- asta::vins %>% rename(target = quality)
# data <- asta::grandile %>% rename(target = PAUVRE) %>%
#   select(-starts_with("LIB")) %>%
#   select(-IDENT) %>%
#   mutate(target = as.factor(target))
```

### Exploration de la base brute

```{r}
skim(data)
```

### Partitionnement de la base brute

La base intiale est découpée en trois bases :

-   base d'entraînement

-   base de validation

-   base de test

Il est possible d'utiliser une variable de stratification (souvent la variable à expliquer) pour que les 3 bases aient suffisament d'individus du même type. Les fonctions utilisées sont issues du package `rsample`.

```{r}
#Paramétrage des parts pour chaque base. 
part_training <- 0.7 #70% pour l'entraînement
part_validation <- (1-part_training)/2 #15% pour la validation


#on contrôle l'aléatoire pour avoir toujours la même base.
set.seed(1234)

#le paramètre strata permet de garder la même proportion de la variable dans les trois bases.
data_split <- initial_validation_split(data,
                                       strata = target, 
                                       prop = c(part_training,part_validation)
                                       )

#La dernière base est la base d'entraînement et la base de validation.
train_data <- training(data_split) 
test_data <- testing(data_split) 
valid_data <- validation(data_split)
train_valid_data <- train_data %>% bind_rows(valid_data)
```

### Modifications de la base

::: callout-tip
C'est le package `recipes` qui est utilisé ici pour faire des modifications sur les bases (du preprocessing) qui peuvent être de nombreux types :

-   ajout/suppresion de variables

-   transformation de variable (log, exponentielle..)

-   imputation des données manquantes (plusieurs méthodes)

-   filtrage d'individus

-   encodage (one hot par exemple..)

Ces modifications sont parfois nécessaires pour que les modèles puissent marcher (par exemple, la forêt aléatoire ne peut pas s'ajuster sur une base avec des données manquantes). Mais elles peuvent aussi permettre d'améliorer la qualité du modèle.

Il y a certaines transformations qui sont automatiquement appliquées aux bases de données par les modèles.

Le package `recipes` permet ainsi de tester plusieurs versions de la base (plusieurs recettes appliquées à la base) sur lesquelles on ajustera un même modèle.

<https://recipes.tidymodels.org/reference/index.html#basic-functions>
:::

```{r}

#choisir les recettes qu'on applique à la base, avec des step
rec <- 
  recipe(target ~ ., data = train_data) %>% 
  step_normalize(all_numeric_predictors()) #%>% 

```

Cette base modifiée peut-être visualisée à l'aide des fonctions contenues dans le package `recipes`. Dans cet exemple, on a choisi de centrer-réduire toutes les variables numériques avec `step_normalize`.

```{r}
recette <- prep(rec) #Je prépare la recette
data_modifie <- bake(recette,new_data = NULL) #Je l'appplique par défaut sur la base définie dans la fonction recipe
skim(data_modifie)
```

### Choix d'un modèle

Il s'agit maintenant de tester un certain nombre de modèles qui sont des cadres d'apprentissage, et qui seront entraînés dans la phase suivante. Il en existe un grand nombre.

::: callout-tip
## Paramètres

Les modèles utilisés viennent du package `parnsnip`. Il faut préciser le nom du modèle, puis le moteur qui va servir à estimer les paramètres (`set_engine`) et les éventuels paramètres et hyper-paramètres à régler.

Pour certains modèles, il faut préciser qu'il s'agit d'un problème de classification (`set_mode`)
:::

-   La régression logistique

```{r}
mod_lr <- 
  logistic_reg() %>% 
  set_engine("glm")
```

-   L'arbre de classification

```{r}
mod_tree <- 
  decision_tree(
    cost_complexity = 0.001, #hyper-paramètre mesurant la complexité de l'arbre
    tree_depth = 7, #pour paramétrer la profondeur de l'arbre
    min_n = NULL #nombre minimum d'individus dans un noeud pour être divisé
  ) %>%
  set_engine("rpart") %>% #méthode d'estimation
  set_mode("classification")
```

-   La forêt aléatoire

    ```{r}
    mod_rf <- 
      rand_forest(trees = 1000, #nombre d'arbres
                  mtry = 3, #nombre de variables
                  min_n = NULL) %>% #nombre minimum d'individus dans un noeud pour être divisé
      set_engine("ranger") %>% 
      set_mode("classification")
    ```

-   Le KNN : plus proches voisins

    ```{r}
    mod_knn <- 
      nearest_neighbor(
        neighbors = 3 #nombre de voisins
      ) %>% 
      set_engine("kknn") %>% 
      set_mode("classification")
    ```

-   Le SVM (support vector machine)[^1]

    ```{r}
    mod_svm <- 
      svm_rbf(mode = "classification", 
                         cost = 10, 
                         rbf_sigma = 0.1, 
                         margin = 1) %>%
      set_engine("kernlab")
    ```

-   La régression ridge[^2]

    ```{r}
    mod_ridge <- 
      logistic_reg(penalty = 0.01, #hyper-paramètre  
                   mixture = 0) %>% 
      set_engine("glmnet")

    ```

-   la régression lasso[^3]

    ```{r}
    mod_lasso <- 
      logistic_reg(penalty = 0.001, #hyper-paramètre
                   mixture = 1) %>% 
      set_engine("glmnet")

    ```

[^1]: modèle non proposé dans ASTA

[^2]: Modèle qui n'est pas proposé dans ASTA

[^3]: modèle non proposé dans ASTA

### Création d'un workflow

::: callout-tip
## Paramètres

Un workflow est une recette appliquée à une base et à un modèle. C'est le package `workflow` qui permet de créer un workflow.
:::

```{r}
mod <- mod_rf

wflow <-  workflow() %>% 
  add_model(mod) %>% #ajout du modèle
  add_recipe(rec) #ajout de la recette (transfo de la base initiale)
wflow
```

### Ajustement du modèle

C'est à cette étape que le modèle s'ajuste sur les données d'entraînement, c'est à dire que l'algorithme trouve le modèle qui estime le mieux. La fonction `fit` génère une instance du modèle qui a été entraîné sur la base d'entraînement.

```{r}
fit <- wflow %>% fit(data=train_data)
extract_fit_parsnip(fit)
```

On peut visualiser les estimations de ce modèle sur la base d'entraînement. A chaque classe de la variable target est associée une probabilité : si elle est supérieure à 0,5, alors c'est cette modalité qui est estimée par le modèle.

```{r}
augment(fit,train_data) %>% select(target,starts_with(".pred")) %>% head(10)
```

### Performance du modèle

L'instance du modèle généré à l'étape précédente est appliquée à la base de validation, qui n'a pas servi à ajuster le modèle. Autrement dit, ce sont des données que le modèle n'a jamais vu et qui vont permettre d'évaluer les capacités prédictives de notre modèle.

::: callout-note
En pratique, on ne fait pas de la validation simple comme ici mais de la validation croisée, qui permet de réduire le biais d'échantillonage au moment de la partition de la base brute.

C'est la fonction `vfold_cv` du package rsample qui permet de paramétrer la validation croisée. Et la fonction `fit_resamples` du package `tune` permet de faire les estimations en utilisant la validation croisée.
:::

```{r}
pred_valid <- augment(fit, valid_data) %>% select(target,starts_with(".pred"))
pred_valid
```

A partir de ce tableau de prédiction sur la base de validation, je peux construire un tableau de confusion, qui classe chaque individu de la base de validation dans une des 4 cases du tableau : VP (vrai positif), FP (faux positif), VN (vrai négatif), FN (faux négatif).

```{r}
pred_valid %>%
  conf_mat(target, .pred_class) %>%
  autoplot(type="heatmap")
```

A partir de ce tableau de confusion, on peut définir des indicateurs de performance et la courbe ROC. Les fonctions permettant de calculer les performances sont dans le package `yardstick`.

```{r}
accuracy <- pred_valid %>% 
  accuracy(truth = target, .pred_class) %>% 
  select(.estimate) %>% 
  as.numeric() %>% 
  round(2)

specificity <- pred_valid %>% 
  specificity(truth = target, .pred_class) %>% 
  select(.estimate) %>% 
  as.numeric() %>% 
  round(2)

sensitivity <- pred_valid %>% 
  sensitivity(truth = target, .pred_class) %>% 
  select(.estimate) %>% 
  as.numeric() %>% 
  round(2)
```

| Modèle            | Recette                       | Accuracy     | Spécificité     | Sensibilité     |
|-------------------|-------------------------------|--------------|-----------------|-----------------|
| `r class(mod)[1]` | `r class(recette$steps[[1]])` | `r accuracy` | `r specificity` | `r sensitivity` |

On peut aussi calculer l'AUC et dessiner la courbe ROC.

```{r}
pred1 <- names(pred_valid)[3]
roc_plot_valid <- pred_valid %>% 
  roc_curve(truth = target,all_of(pred1)) %>% 
  autoplot()
roc_plot_valid
```

### Ajustement du modèle final

Le modèle final (celui qui a la meilleure performance) est ajusté sur la base d'entraînement et la base de validation. C'est cette instance de modèle qui sera utilisé pour faire des prédictions sur des individus qui n'ont jamais été vus par le modèle.

```{r}
fit_final <-
  wflow %>% fit(train_valid_data)
pred_final <- augment(fit_final,test_data)

accuracy <- pred_final %>% 
  accuracy(truth = target, .pred_class) %>% 
  select(.estimate) %>% 
  as.numeric() %>% 
  round(2)

specificity <- pred_final %>% 
  specificity(truth = target, .pred_class) %>% 
  select(.estimate) %>% 
  as.numeric() %>% 
  round(2)

sensitivity <- pred_final %>% 
  sensitivity(truth = target, .pred_class) %>% 
  select(.estimate) %>% 
  as.numeric() %>% 
  round(2)
```

| Modèle            | Recette                       | Accuracy     | Spécificité     | Sensibilité     |
|-------------------|-------------------------------|--------------|-----------------|-----------------|
| `r class(mod)[1]` | `r class(recette$steps[[1]])` | `r accuracy` | `r specificity` | `r sensitivity` |

## Régression supervisée

Les deux fichiers utilisés sont :

-   le fichier ozone (on cherche à prédire le taux d'ozone)

-   le fichier Grandile (on cherche à prédir le taux de pauvreté)

```{r}

data <- ozone %>%
  mutate(target = maxO3) %>%
  select(-maxO3)


# data <- grandile  %>%
#   select(-starts_with("LIB")) %>%
#   mutate(target = REV_DISPONIBLE) %>%
#   select(-IDENT,-REV_DISPONIBLE)
```

### Exploration de la base brute

```{r}
skim(data)
```

Dans la base de données brute d'ozone, on constate qu'il y a deux valeurs manquantes sur la variable Ne18. Il faudra en tenir compte dans les phases suivantes.

### Partitionnement de la base brute

La base intiale est découpée en trois bases :

-   base d'entraînement

-   base de validation

-   base de test

::: callout-tip
## Paramètres

Les fonctions permettant de partitionner la base de départ sont issues de `rsample`. La fonction initial_validation_split divise la base de départ en 3.
:::

```{r}
part_training <- 0.7
part_validation <- (1-part_training)/2 

set.seed(1234) #pour permettre la reproductibilité des résultats
data_split <- initial_validation_split(data,prop = c(part_training,part_validation)) 

train_data <- training(data_split) #le fichier d'entraînement
test_data <- testing(data_split) #le fichier de test
valid_data <- validation(data_split)
train_valid_data <- train_data %>% bind_rows(valid_data)
```

### Modifications de la base

Il est rare que la base brute puisse directement être traitée par les modèles. Elle nécessite une phase (souvent longue) de nettoyage, de recodage, de discrétisation...

Cette première phase permet de préparer la modélisation. Il est important de noter que les nouvelles données qu'on cherchera à prédire avec le modèle auront ce même format brut, et qu'elle subiront donc les mêmes modifications avant de rentrer dans le modèle.

::: callout-tip
## Paramètres

Le package `recipes` permet d'appliquer des recettes sur la base brute, c'est à dire des suites de traitement à appliquer à cette base. Cette étape s'appelle aussi le preprocessing, ou feature eigeneering.
:::

Avec la base de données `ozone`, on choisit de faire de l'imputation de valeurs manquantes par la moyenne (`step_impute_mean`). On aurait pu aussi filtrer les valeurs manquantes ou utiliser une autre techique d'imputation (médiane, plus proches voisins...).

```{r}
rec <- recipe(target ~ ., data = train_data) %>% 
  step_impute_mean(all_numeric_predictors()) #imputation valeurs manquantes
```

Une fois que la recette est enregistrée (liste des traitements à appliquer à la base brute), on peut afficher la base de données avec les verbes `prep` et `bake`.

```{r}
recette <- prep(rec) #Je prépare la recette
data_modifie <- bake(recette,new_data = NULL) #Je l'appplique par défaut sur la base définie dans la fonction recipe
skim(data_modifie)
```

### Choix d'un modèle

::: callout-tip
## Paramètres

Les modèles utilisés viennent du package `parnsnip`. Il faut préciser le nom du modèle, puis le moteur qui va servir à estimer les paramètres (`set_engine`) et les éventuels paramètres et hyper-paramètres à régler.
:::

-   La régression linéaire

```{r}
mod_lr <- linear_reg() %>% 
  set_engine("lm") 
```

-   L'arbre de décision

```{r}
mod_tree <- 
  mod_tree <- 
  decision_tree(
    cost_complexity = 0.001,
    tree_depth = 7,
    min_n = NULL
  ) %>%
  set_engine("rpart") %>%
  set_mode("regression")
```

-   La forêt aléatoire

```{r}
mod_rf <- rand_forest() %>%
  set_engine("ranger") %>%
  set_mode("regression")
```

-   Le SVM (support vector machine)

```{r}
mod_svm <- svm_linear() %>%
  set_mode("regression") %>%
  set_engine("LiblineaR")
```

-   Le KNN

```{r}

#Modèle KNN
mod_knn <- 
  nearest_neighbor(
    neighbors = 3
  ) %>% 
  set_engine("kknn") %>% 
  set_mode("regression")

```

### Création d'un workflow

::: callout-tip
## Paramètres

Un workflow est une recette appliquée à une base et à un modèle. C'est le package `workflow` qui permet de créer un workflow.
:::

```{r}
mod <- mod_lr

wflow <- workflow() %>% 
  add_recipe(rec) %>% 
  add_model(mod_lr)
wflow
```

### Ajustement du modèle

On peut maintenant faire tourner l'alogorithme choisi pour ajuster le modèle sur les données d'entraînement auxquelles on a appliqué une recette (le workflow).

```{r}
fit <- wflow %>% fit(data=train_data)
extract_fit_parsnip(fit)
```

On peut visualiser les estimations de ce modèle sur la base d'entraînement. Moins les écarts entre les estimations et les vraies valeurs sont élevés, meilleur est le modèle.

```{r}
pred_train <- augment(fit,train_data) %>% select(target,.pred) %>% round(1)
pred_train %>% head(10)
```

### Performance du modèle

L'instance du modèle généré à l'étape précédente est appliquée à la base de validation, qui n'a pas servi à ajuster le modèle. Autrement dit, ce sont des données que le modèle n'a jamais vu et qui vont permettre d'évaluer les capacités prédictives de notre modèle. Cette étape permet de s'assurer qu'il n'y a pas de surraprentissage.

```{r}
pred_valid <- augment(fit,valid_data) %>% select(target,.pred) %>% round(1)
pred_valid %>% head(10)
```

A partir de ce tableau de prédiction sur la base de validation, je peux calculer des indicateurs qui mesurent la distance entre la valeur réelle et la valeur prédite par une instance du modèle retenu.

::: callout-tip
## Paramètres

Le package `yardstick` calcule automatiquement les indicateurs de performances, avec des fonctions pré-enregistrées.
:::

```{r}
#risque quadratique
rss <- sum((pred_valid$.pred - pred_valid$target)^2) %>% round(1)

mse <- rss/nrow(pred_valid) %>% round(1)

rmse <- sqrt(mse) %>% round(1)
# rmse(pred_valid,target,.pred) %>% select(.estimate) %>% as.numeric() %>% round(2)

#coefficient de détermination
rsq <- rsq(pred_valid,target,.pred) %>% select(.estimate) %>% as.numeric() %>% round(2)
```

| Modèle            | Recette                       | MSE     | RMSE     | R²      |
|-------------------|-------------------------------|---------|----------|---------|
| `r class(mod)[1]` | `r class(recette$steps[[1]])` | `r mse` | `r rmse` | `r rsq` |

Dans cet exemple, on a testé les performances de la régression linéaire sur toutes les variables. Il faut bien sûr comparer ces performances avec d'autres modèles pour prendre le meilleur.

### Ajustement du modèle final

Le modèle final (celui qui a été retenu à la phase précédente) est ajusté sur la base d'entraînement et la base de validation.

```{r}
fit_final <- 
  wflow %>% 
  fit(train_valid_data)

```

On utilise cette nouvelle instance pour prédire sur la base de test, c'est à dire sur la base qui n'a pas été utilisée pour choisir le modèle et les hyper-paramètres.

```{r}
pred_testing <- augment(fit_final, test_data) %>% select(target,.pred)
rss <- sum((pred_testing$.pred - pred_testing$target)^2) %>% round(1)

mse <- rss/nrow(pred_testing) %>% round(1)

rmse <- sqrt(mse) %>% round(1)
# rmse(pred_valid,target,.pred) %>% select(.estimate) %>% as.numeric() %>% round(2)

#coefficient de détermination
rsq <- rsq(pred_testing,target,.pred) %>% select(.estimate) %>% as.numeric() %>% round(2)
```

| Modèle            | Recette                       | MSE     | RMSE     | R²      |
|-------------------|-------------------------------|---------|----------|---------|
| `r class(mod)[1]` | `r class(recette$steps[[1]])` | `r mse` | `r rmse` | `r rsq` |
